{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "wjT1m9IJuoJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "def get_cik(ticker):\n",
        "    company_name = ticker\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
        "    })\n",
        "\n",
        "    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
        "    params = {\n",
        "        \"action\": \"getcompany\",\n",
        "        \"CIK\": ticker,\n",
        "        \"output\": \"xml\"\n",
        "    }\n",
        "\n",
        "    response = session.get(base_url, params=params)\n",
        "\n",
        "    print(f\"URL used for request: {response.url}\")\n",
        "    print(f\"Response Status Code: {response.status_code}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "        cik_element = root.find('.//CIK')\n",
        "        if cik_element is not None:\n",
        "            if company_name == 'shortName not found':\n",
        "                company_name = root.find('.//name').text\n",
        "            return cik_element.text, company_name\n",
        "        else:\n",
        "            return \"CIK not found\", company_name\n",
        "    else:\n",
        "        return \"Request failed with status code \" + str(response.status_code), company_name\n",
        "\n",
        "# # Example usage\n",
        "ticker_symbol = \"META\"\n",
        "cik_number, company_name = get_cik(ticker_symbol)\n",
        "print(f\"CIK Number for {company_name}: {cik_number}\")\n",
        "print(f\"Ticker Symbol for {company_name}: {ticker_symbol}\")\n"
      ],
      "metadata": {
        "id": "XQCtEKccj0Fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de55630-b755-431a-d973-bcb3b789159e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL used for request: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=META&output=xml\n",
            "Response Status Code: 200\n",
            "CIK Number for META: 0001326801\n",
            "Ticker Symbol for META: META\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def load_10k_xbrl(cik_num=None, years=None):\n",
        "    if cik_num is None:\n",
        "        print(\"CIK number is required.\")\n",
        "        return []\n",
        "\n",
        "    url_to_all_10k = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik_num}&type=10-K&dateb=&owner=include&count=40&search_text=\"\n",
        "\n",
        "    # Handle 'years' input\n",
        "    if years is None:\n",
        "        print(\"No specific years are given, so all xbrl urls will be returned.\")\n",
        "        return []\n",
        "    elif isinstance(years, str):\n",
        "        years = [years]\n",
        "    elif not isinstance(years, set):\n",
        "        years = set(years)\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url_to_all_10k, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        table = soup.find('table', class_='tableFile2')\n",
        "\n",
        "        if table:\n",
        "            full_links = []\n",
        "            for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
        "                cols = row.find_all('td')\n",
        "                if len(cols) > 3:\n",
        "                    filing_type = cols[0].text.strip()\n",
        "                    filing_date = cols[3].text.strip()\n",
        "\n",
        "                    if filing_type == '10-K' and any(target_year in filing_date for target_year in years):\n",
        "                        doc_link = cols[1].find('a', href=True)['href']\n",
        "                        full_links.append(f\"https://www.sec.gov{doc_link}\")\n",
        "            if full_links:\n",
        "                return full_links\n",
        "            else:\n",
        "                print(\"No 10-K filings found for the specified years.\")\n",
        "                return []\n",
        "        else:\n",
        "            print(\"No table found on the SEC page.\")\n",
        "            return []\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Network error: {e}\")\n",
        "        return []\n",
        "\n",
        "# # Example usage\n",
        "# urls = load_10k_xbrl(cik_number, [\"2019\",\"2020\",\"2021\",\"2022\", \"2023\"])\n",
        "# print(urls)\n"
      ],
      "metadata": {
        "id": "6plvCPwGj0IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Function to get XBRL file links from the given SEC page link\n",
        "def get_xbrl_links(link):\n",
        "    session = requests.Session()  # Start a new session\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
        "    }\n",
        "    session.headers.update(headers)  # Set headers for the session\n",
        "\n",
        "    response = session.get(link)  # Get the content of the SEC page\n",
        "    print(f\"Accessing URL: {link}\")\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "    file_links = []  # List to store the XBRL file links\n",
        "    folder_name = None\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')  # Parse the page content\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            file_link = urljoin(link, a_tag['href'])  # Create full URL for the file link\n",
        "\n",
        "            if file_link.endswith(('.xml', '.xsd')):  # Check if the link is a downloadable file\n",
        "                print(f\"Found file link: {file_link}\")\n",
        "                folder_name_new = file_link.split('/')[-1]\n",
        "                folder_name_new = re.split(r'[._]', folder_name_new)[0]\n",
        "\n",
        "                if folder_name is None:\n",
        "                    folder_name = folder_name_new\n",
        "\n",
        "                file_links.append(file_link)  # Add the file link to the list\n",
        "    else:\n",
        "        print(\"Failed to retrieve the webpage\")\n",
        "\n",
        "    return file_links, folder_name\n",
        "\n",
        "# Function to download a file given its link\n",
        "def download_file(session, file_link, folder_name):\n",
        "    for attempt in range(3):  # Try to download up to 3 times\n",
        "        file_response = session.get(file_link)\n",
        "        if file_response.status_code == 200:\n",
        "            file_name = file_link.split('/')[-1]\n",
        "            file_path = os.path.join(folder_name, file_name)\n",
        "            with open(file_path, 'wb') as file:\n",
        "                file.write(file_response.content)  # Write the file content\n",
        "            print(f\"Downloaded: {file_path}\")\n",
        "            return file_path\n",
        "        else:\n",
        "            print(file_response.status_code)\n",
        "            print(f\"Failed to download file: {file_link} (Attempt {attempt + 1})\")\n",
        "    return None\n",
        "\n",
        "# Main function to handle the downloading of SEC files\n",
        "def download_sec_files(link, download_dir):\n",
        "    file_links, folder_name = get_xbrl_links(link)  # Get XBRL links\n",
        "\n",
        "    if not file_links:\n",
        "        print(\"No files to download\")\n",
        "        return folder_name, None\n",
        "\n",
        "    session = requests.Session()  # Start a new session\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
        "    }\n",
        "    session.headers.update(headers)  # Set headers for the session\n",
        "\n",
        "    if not os.path.exists(os.path.join(download_dir, folder_name)):\n",
        "        os.makedirs(os.path.join(download_dir, folder_name))  # Create folder if it doesn't exist\n",
        "\n",
        "    xbrl_name = None\n",
        "\n",
        "    for file_link in file_links:\n",
        "        downloaded_file_path = download_file(session, file_link, os.path.join(download_dir, folder_name))\n",
        "        if downloaded_file_path and '_htm.xml' in downloaded_file_path:\n",
        "            xbrl_name = downloaded_file_path  # Update xbrl_name if the file is the main XBRL file\n",
        "\n",
        "    return folder_name, xbrl_name\n",
        "\n",
        "# Example usage\n",
        "# download_dir = os.path.abspath(\"downloads\")  # Define your download directory\n",
        "# sec_link = 'https://www.sec.gov/Archives/edgar/data/1326801/000132680124000012/0001326801-24-000012-index.htm'\n",
        "\n",
        "# folder, xbrl_file = download_sec_files(urls[1], download_dir)\n",
        "# print(f\"Folder: {folder}, XBRL File: {xbrl_file}\")\n"
      ],
      "metadata": {
        "id": "V-i5d-Usj0K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usage"
      ],
      "metadata": {
        "id": "Vvru9b6Pur58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of tickers and years\n",
        "tickers = [\"AMGN\", \"BA\", \"CAT\", \"JNJ\"]\n",
        "years = set([\"2023\",\"2024\"]) # Year of filing date\n",
        "\n",
        "# Loop through each ticker and year to download XBRL files\n",
        "for ticker in tickers:\n",
        "    print(f\"\\n\\nProcessing ticker: {ticker}\")\n",
        "    cik_number, company_name = get_cik(ticker)\n",
        "    print(f\"CIK Number for {company_name}: {cik_number}\")\n",
        "\n",
        "    urls = load_10k_xbrl(cik_number, years)\n",
        "    print(f\"URLs for {company_name}: {urls}\")\n",
        "\n",
        "    download_dir = '.'  # Current working directory\n",
        "    for url, year in zip(urls, years):\n",
        "        print('\\nProcessing year:', year)\n",
        "        folder, xbrl_file = download_sec_files(url, download_dir)\n",
        "        print(f\"Folder: {folder}, XBRL File: {xbrl_file}\")"
      ],
      "metadata": {
        "id": "5GRNIZG8j0NV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}