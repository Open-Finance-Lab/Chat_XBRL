{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", token='')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", token='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the device (use GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a response for a given question using the model.\n",
    "  Args:\n",
    "    question (str): The question to ask.\n",
    "    context (str): Contextual information for the question.\n",
    "    instructions (str): Additional instructions for the response.\n",
    "    tokenizer: Tokenizer for input preparation.\n",
    "    model: Pre-trained model for response generation.\n",
    "\n",
    "  Returns:\n",
    "    str: Generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ask_question(question, context, instructions, tokenizer, model):\n",
    "    # Format the input prompt\n",
    "    prompt = f\"Context: {context}\\n\\nInstructions: {instructions}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate responses for a list of queries.\n",
    "  Args:\n",
    "    json_data (list): List of dictionaries containing query information.\n",
    "\n",
    "  Returns:\n",
    "    list: List of responses with query and corresponding answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_responses(json_data):\n",
    "  responses = []\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  for item in json_data:\n",
    "    # Call the ask_question function to get a response\n",
    "    answer = ask_question(\n",
    "      item['Query'],\n",
    "      item['Context'],\n",
    "      item.get('Additional Instructions', ''),\n",
    "      tokenizer,\n",
    "      model\n",
    "    )\n",
    "    responses.append({\n",
    "      \"query\": item['Query'],\n",
    "      \"response\": answer\n",
    "    })\n",
    "  return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "json_file = \"/formula_calculation.json\"\n",
    "df = pd.read_json(json_file)\n",
    "json_data = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a single item from the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "first_item = json_data[0]\n",
    "answer = ask_question(\n",
    "  first_item['Query'],\n",
    "  first_item['Context'],\n",
    "  first_item.get('Additional Instructions', ''),\n",
    "  tokenizer,\n",
    "  model\n",
    ")\n",
    "print(\"Query:\", first_item['Query'])\n",
    "print(\"Additional Instructions:\", first_item.get('Additional Instructions', ''))\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate responses for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_data = generate_responses(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save responses to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_file = \"generated_responses.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "  json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Responses have been saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
