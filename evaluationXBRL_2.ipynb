{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticate using Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "login(token='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer and model from Hugging Face Hub\n",
    "# Ensure authentication for private models if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", token='your_hf_token')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", token='your_hf_token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device (GPU if available, else CPU)\n",
    "# Clear any cached GPU memory\n",
    "# Move model to the selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() \n",
    "model.to(device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate responses using the model\n",
    "Generates responses for input data in JSON format.\n",
    "    \n",
    "  Args:\n",
    "    json_data (list): List of queries with context and additional instructions.\n",
    "    max_length (int): Maximum length of the generated response.\n",
    "    batch_size (int): Number of queries to process per batch.\n",
    "\n",
    "  Returns:\n",
    "    list: Generated responses with queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_responses(json_data, max_length=20, batch_size=5):\n",
    "  responses = []  # Store generated responses\n",
    "  tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "\n",
    "  # Prepare input queries with context and additional instructions\n",
    "  queries_with_context = [\n",
    "    f\"Answer the following in strict format as: {item['Response Formats']}.\\n\"\n",
    "    f\"Question: {item['Query']}\\n\"\n",
    "    f\"Context: {item['Context']}\\n\"\n",
    "    f\"Additional Instructions: {item.get('Additional Instructions', '')}\"\n",
    "    for item in json_data\n",
    "  ]\n",
    "\n",
    "  # Process queries in batches\n",
    "  for i in range(0, len(queries_with_context), batch_size):\n",
    "    batch_questions = queries_with_context[i:i + batch_size]\n",
    "\n",
    "    # Tokenize input queries\n",
    "    inputs = tokenizer(\n",
    "      batch_questions,\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate responses from the model\n",
    "    outputs = model.generate(\n",
    "      input_ids=inputs['input_ids'],\n",
    "      attention_mask=inputs['attention_mask'],\n",
    "      max_new_tokens=max_length,\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "      no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    # Decode model outputs and map to original queries\n",
    "    for output, item in zip(outputs, json_data[i:i + batch_size]):\n",
    "      decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "      responses.append({\n",
    "        \"query\": item['Query'],\n",
    "        \"response\": decoded_output\n",
    "      })\n",
    "\n",
    "  return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the input data from a JSON file\n",
    "# Convert DataFrame to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "json_file = \"./ValueTest.json\"\n",
    "df = pd.read_json(json_file)\n",
    "json_data = df.to_dict(orient='records')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate responses and save the results to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_data = generate_responses(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('generated_responses.json', 'w') as f:\n",
    "    pd.json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"Responses have been saved to 'generated_responses.json'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
